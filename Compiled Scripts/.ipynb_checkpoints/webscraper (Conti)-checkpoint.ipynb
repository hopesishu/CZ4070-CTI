{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conti Ransomware Scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "\n",
    "import socks\n",
    "import socket\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring Socks to use Tor\n",
    "\n",
    "from urllib.request import urlopen\n",
    "socks.set_default_proxy(socks.SOCKS5, \"localhost\", 9150)\n",
    "\n",
    "old_socket = socket.socket # saving old socket info \n",
    "socket.socket = socks.socksocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is necessary to use Tor for DNS resolution of Onion websites\n",
    "\n",
    "def getaddrinfo(*args):\n",
    "    return [(socket.AF_INET, socket.SOCK_STREAM, 6, '', (args[0], args[1]))]\n",
    "\n",
    "socket.getaddrinfo = getaddrinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using requests package to read in the Hidden Wiki Onion Website on the Darknet\n",
    "\n",
    "conti_link = \"http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/\"\n",
    "res = requests.get(conti_link)\n",
    "\n",
    "# Using beautifulsoup to get the website content into a nice format\n",
    "soup = BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/',\n",
       " 'https://continews.click',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion',\n",
       " 'https://www.officestarstore.com',\n",
       " '/5lqBdb_Office_Star_Products',\n",
       " 'https://taxleaf.com/',\n",
       " '/lxmtS3_TaxLeaf_Corporate',\n",
       " '/irlRNE_JVCKenwood_Case',\n",
       " 'https://www.westtree.com',\n",
       " '/oNTEv3_WEST_TREE_SERVICE',\n",
       " 'https://www.gershman.com',\n",
       " '/GIOdwq_Gershman_Mortgage',\n",
       " 'https://www.rtcnt.com',\n",
       " '/aoQuvx_Real_Time',\n",
       " 'https://covisian.com',\n",
       " '/VwCUtB_Grupo_GSS',\n",
       " 'https://minjargold.com.au',\n",
       " '/SAFsv5_Minjar_Gold',\n",
       " 'https://www.parcoinc.com/',\n",
       " '/krqH6c_parcoinc',\n",
       " '/page/1',\n",
       " '/page/2',\n",
       " '/page/3',\n",
       " '/page/4',\n",
       " '/page/5',\n",
       " '/page/6',\n",
       " '/page/7',\n",
       " '/page/8',\n",
       " '/page/9',\n",
       " '/page/2',\n",
       " '/page/54']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting all links out of the soup and deleting None's\n",
    "\n",
    "links = [link.get('href') for link in soup.find_all('a', href=True)]\n",
    "links = list(filter(None, links))\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/1',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/2',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/3',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/4',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/5',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/6',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/7',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/8',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/9',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/10',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/11',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/12',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/13',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/14',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/15',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/16',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/17',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/18',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/19',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/20',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/21',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/22',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/23',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/24',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/25',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/26',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/27',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/28',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/29',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/30',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/31',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/32',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/33',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/34',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/35',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/36',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/37',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/38',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/39',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/40',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/41',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/42',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/43',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/44',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/45',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/46',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/47',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/48',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/49',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/50',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/51',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/52',\n",
       " 'http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/53']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually creating the links as the scraping of links have some issue.\n",
    "\n",
    "conti_link_manual = \"http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/\"\n",
    "webpage_links = [] \n",
    "for x in range (1,54):\n",
    "    str_convert = str(x)\n",
    "    final = conti_link_manual + str_convert\n",
    "    webpage_links.append(final)\n",
    "        \n",
    "webpage_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find all company that are affected with conti ransomware\n",
    "company_list = []\n",
    "for link in webpage_links:\n",
    "    res = requests.get(link)\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    posts = soup.findAll(\"div\", {\"class\": \"title\"})\n",
    "    for post in posts:\n",
    "        l = post.text\n",
    "        cleaned = l.replace('“', '').replace('”','')\n",
    "        company_list.append(cleaned)\n",
    "\n",
    "company_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all dates that company was affected with ransomware \n",
    "date_list = []\n",
    "for link in webpage_links:\n",
    "    res = requests.get(link)\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    posts = soup.findAll(\"div\", {\"class\": \"footer\"})\n",
    "    for post in posts:\n",
    "        l = post.text\n",
    "        split_string = l.split(\",\", 1)\n",
    "        substring = split_string[0]\n",
    "        cleaned = substring.replace('\\n','')\n",
    "        date_list.append(cleaned)\n",
    "\n",
    "date_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Selenium web automation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(columns=['Companies', 'Industries','Affected_Date'])\n",
    "df['Companies'] = company_list\n",
    "df['Affected_Date'] = date_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install webdriver-manager\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "socket.socket = old_socket # reuse the original socket for selenium\n",
    "\n",
    "s=Service(ChromeDriverManager().install())\n",
    "print(s)\n",
    "\n",
    "driver = webdriver.Chrome(service=s)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia Search\n",
    "industry_list = []\n",
    "number = 0\n",
    "\n",
    "#For ChromeDriver version 79.0.3945.16 or over\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--disable-blink-features=AutomationControlled') \n",
    "options.add_argument(\"enable-automation\");\n",
    "options.add_argument(\"--headless\");\n",
    "options.add_argument(\"--window-size=1920,1080\");\n",
    "options.add_argument(\"--no-sandbox\");\n",
    "options.add_argument(\"--disable-extensions\");\n",
    "options.add_argument(\"--dns-prefetch-disable\");\n",
    "options.add_argument(\"--disable-gpu\");\n",
    "\n",
    "for company in company_list:\n",
    "    driver.get('https://www.google.com')\n",
    "    search = driver.find_element(By.NAME, 'q')\n",
    "    search.send_keys(company + ' wikipedia')\n",
    "    search.send_keys(Keys.RETURN) # hit return after you enter search text\n",
    "\n",
    "    wiki_page = driver.find_element_by_tag_name('h3') # clicking the first search result\n",
    "    wiki_page.click()\n",
    "\n",
    "    try:\n",
    "        infobox = driver.find_elements_by_class_name(\"infobox-label\")\n",
    "        infobox_data = driver.find_elements_by_class_name(\"infobox-data\")\n",
    "\n",
    "        index = 0\n",
    "        for info in infobox:\n",
    "            #print(info.text)\n",
    "            if (info.text == \"Industry\"):\n",
    "                industry_list.append(infobox_data[index].text)\n",
    "                print(str(number) + \") \" + company + \": \" + infobox_data[index].text)\n",
    "                break \n",
    "            index = index + 1\n",
    "        else:\n",
    "            industry_list.append(np.nan)\n",
    "            print(str(number) + \") \" + company + \": \" + str(np.nan))\n",
    "    except:\n",
    "        print(\"none\")\n",
    "    number = number + 1\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glassdoor search\n",
    "industry_list = []\n",
    "number = 0\n",
    "\n",
    "#For ChromeDriver version 79.0.3945.16 or over\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--disable-blink-features=AutomationControlled') \n",
    "options.add_argument(\"enable-automation\");\n",
    "options.add_argument(\"--headless\");\n",
    "options.add_argument(\"--window-size=1920,1080\");\n",
    "options.add_argument(\"--no-sandbox\");\n",
    "options.add_argument(\"--disable-extensions\");\n",
    "options.add_argument(\"--dns-prefetch-disable\");\n",
    "options.add_argument(\"--disable-gpu\");\n",
    "\n",
    "for company in comp_list: \n",
    "    driver.get('https://www.google.com')\n",
    "    search = driver.find_element(By.NAME, 'q')\n",
    "    search.send_keys(company + ' glassdoor \"overview\"')\n",
    "    search.send_keys(Keys.RETURN)\n",
    "    glassdoor_page = driver.find_element_by_tag_name('h3') # clicking the first search result\n",
    "    glassdoor_page.click()\n",
    "        \n",
    "    try:\n",
    "        pagesource = driver.page_source\n",
    "        soup = BeautifulSoup(pagesource, 'html.parser')\n",
    "        labels = soup.find_all('label')\n",
    "        for label in labels:\n",
    "            if (label.text == \"Industry:\"):\n",
    "                industry = label.next_sibling.text\n",
    "                industry_list.append(industry)\n",
    "                print(str(number) + \") \" + company + \": \" + industry)\n",
    "                break\n",
    "        else:\n",
    "            industry_list.append(np.nan)\n",
    "            print(str(number) + \") \" + company + \": \" + str(np.nan))\n",
    "    except:\n",
    "        print(\"none\")\n",
    "        \n",
    "    number = number + 1\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glassdoor search to find headquarters of victim companies\n",
    "headquarter_list = []\n",
    "number = 0\n",
    "\n",
    "#For ChromeDriver version 79.0.3945.16 or over\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--disable-blink-features=AutomationControlled') \n",
    "options.add_argument(\"enable-automation\");\n",
    "options.add_argument(\"--headless\");\n",
    "options.add_argument(\"--window-size=1920,1080\");\n",
    "options.add_argument(\"--no-sandbox\");\n",
    "options.add_argument(\"--disable-extensions\");\n",
    "options.add_argument(\"--dns-prefetch-disable\");\n",
    "options.add_argument(\"--disable-gpu\");\n",
    "\n",
    "for company in comp_list: \n",
    "    driver.get('https://www.google.com')\n",
    "    search = driver.find_element(By.NAME, 'q')\n",
    "    search.send_keys(company + ' glassdoor \"overview\"')\n",
    "    search.send_keys(Keys.RETURN)\n",
    "    glassdoor_page = driver.find_element_by_tag_name('h3') # clicking the first search result\n",
    "    glassdoor_page.click()\n",
    "        \n",
    "    try:\n",
    "        pagesource = driver.page_source\n",
    "        soup = BeautifulSoup(pagesource, 'html.parser')\n",
    "        labels = soup.find_all('label')\n",
    "        for label in labels:\n",
    "            if (label.text == \"Headquarters:\"):\n",
    "                industry = label.next_sibling.text\n",
    "                headquarter_list.append(industry)\n",
    "                print(str(number) + \") \" + company + \": \" + industry)\n",
    "                break\n",
    "        else:\n",
    "             headquarter_list.append(np.nan)\n",
    "             print(str(number) + \") \" + company + \": \" + str(np.nan))\n",
    "    except:\n",
    "        print(\"none\")\n",
    "    number = number + 1\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Data cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Industries'] = industry_list\n",
    "df = df.dropna() # remove rows with NaN\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True) # reset index\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert states to countries\n",
    "#pip install geopy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data= pd.read_csv('conti1.csv')\n",
    "data\n",
    "\n",
    "from geopy.geocoders import Nominatim \n",
    "geolocator = Nominatim(user_agent = \"geoapiExercises\")\n",
    "num=0\n",
    "country_list=[]\n",
    "\n",
    "\n",
    "for city in data['Headquarters']:\n",
    "    place, (lat, lng) = geolocator.geocode(city)\n",
    "    country = place.split()[-1]\n",
    "    print(country)\n",
    "    country_list.append(country)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
