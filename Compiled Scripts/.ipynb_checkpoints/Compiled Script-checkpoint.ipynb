{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the Dark Web\n",
    "The ransomware websites that we have chosen to scrape are the following:\n",
    "1) REvil\n",
    "2) Conti\n",
    "3) Pysa\n",
    "4) XINGTeam\n",
    "5) BlackMatter\n",
    "6) RansomExx\n",
    "\n",
    "Using BeautifulSoup and Selenium, names of victim companies have been scraped from the dark web, and their respective industries and headquarter locations are scraped from the clear web using Selenium. Additionally, using a Python library \"GeoPy\", the countiries of the victim companies can be located, and used for visualization on Tableau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from geopy.geocoders import Nominatim \n",
    "\n",
    "import socks\n",
    "import socket\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "socks.set_default_proxy(socks.SOCKS5, \"localhost\", 9150)\n",
    "old_socket = socket.socket # saving old socket info \n",
    "socket.socket = socks.socksocket\n",
    "def getaddrinfo(*args):\n",
    "    return [(socket.AF_INET, socket.SOCK_STREAM, 6, '', (args[0], args[1]))]\n",
    "socket.getaddrinfo = getaddrinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Scraping REvil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REvil_link = \"http://dnpscnbaix6nkwvystl3yxglz7nteicqrou3t75tpcc5532cztc46qyd.onion\"\n",
    "res = requests.get(REvil_link)\n",
    "soup = BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting all weblinks into a list \n",
    "links = [link.get('href') for link in soup.find_all('a', href=True)]\n",
    "links = list(filter(None, links))\n",
    "\n",
    "webpage_links = [] \n",
    "for l in links:\n",
    "    if \"page\" in l:\n",
    "        l = REvil_link + l\n",
    "        webpage_links.append(l)\n",
    "        \n",
    "webpage_links = webpage_links[1:-1] # remvoving first and last element from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of victims from REvil\n",
    "REvilHeader_list = []\n",
    "for link in webpage_links:\n",
    "    res = requests.get(link)\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    posts = soup.find_all(class_ = \"blog-post-title\")\n",
    "    \n",
    "    for post in posts:\n",
    "        title = post.find('a')\n",
    "        REvilHeader_list.append(title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placing victims into a DataFrame \n",
    "df = pd.DataFrame(columns=['Company Name', 'Industry', 'Headquarters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Selenium to automate finding industries\n",
    "socket.socket = old_socket # reuse the original socket for selenium\n",
    "s=Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=s)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glassdoor search\n",
    "company_list = []\n",
    "industry_list = []\n",
    "location_list = []\n",
    "number = 0\n",
    "\n",
    "for header in REvilHeader_list: \n",
    "    try:\n",
    "        driver.get('https://www.google.com')\n",
    "        search = driver.find_element(By.NAME, 'q')\n",
    "        search.send_keys(header + ' glassdoor overview')\n",
    "        search.send_keys(Keys.RETURN)\n",
    "        glassdoor_page = driver.find_element_by_tag_name('h3') # clicking the first search result\n",
    "        glassdoor_page.click()\n",
    "        \n",
    "        if \"glassdoor\" in driver.current_url:\n",
    "            company = driver.find_element_by_tag_name('h1')\n",
    "            company_list.append(company.text)\n",
    "            \n",
    "            pagesource = driver.page_source\n",
    "            soup = BeautifulSoup(pagesource, 'html.parser')\n",
    "            labels = soup.find_all('label')\n",
    "            for label in labels:\n",
    "                if (label.text == \"Industry:\"):\n",
    "                    industry = label.next_sibling.text\n",
    "                    industry_list.append(industry)\n",
    "                    break\n",
    "            else:\n",
    "                industry_list.append(np.nan)\n",
    "                \n",
    "            for label in labels:\n",
    "                if (label.text == \"Headquarters:\"):\n",
    "                    location = label.next_sibling.text\n",
    "                    location_list.append(location)\n",
    "                    break\n",
    "            else:\n",
    "                location_list.append(np.nan)\n",
    "        else:\n",
    "            company_list.append(np.nan)\n",
    "            industry_list.append(np.nan)\n",
    "            location_list.append(np.nan)\n",
    "    except Exception as e:\n",
    "        company_list.append(np.nan)\n",
    "        industry_list.append(np.nan)\n",
    "        location_list.append(np.nan)\n",
    "        print(e)\n",
    "\n",
    "    print(str(number) + \") \" + header)\n",
    "    number = number + 1\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placing all scraped data into a DataFrame\n",
    "df['Company Name'] = company_list\n",
    "df['Industry'] = industry_list\n",
    "df['Headquarters'] = location_list\n",
    "\n",
    "# Cleaning data\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True) # reset index\n",
    "\n",
    "df.to_csv('REvil.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding countries of headquarters\n",
    "data = pd.read_csv('Revil.csv')\n",
    "geolocator = Nominatim(user_agent = \"geoapiExercises\")\n",
    "country_list = []\n",
    "\n",
    "for city in data['Headquarters']:\n",
    "    place, (lat, lng) = geolocator.geocode(city)\n",
    "    country = place.split()[-1]\n",
    "    country_list.append(country)\n",
    "    \n",
    "data['Country'] = country_list\n",
    "df.to_csv('REvil.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Scraping Conti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conti_link = \"http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/\"\n",
    "res = requests.get(conti_link)\n",
    "soup = BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually creating the links as the scraping of links have some issue\n",
    "conti_link_manual = \"http://continewsnv5otx5kaoje7krkto2qbu3gtqef22mnr7eaxw3y6ncz3ad.onion/page/\"\n",
    "webpage_links = [] \n",
    "for x in range (1,54):\n",
    "    str_convert = str(x)\n",
    "    final = conti_link_manual + str_convert\n",
    "    webpage_links.append(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all company that are affected with conti ransomware\n",
    "company_list = []\n",
    "for link in webpage_links:\n",
    "    res = requests.get(link)\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    posts = soup.findAll(\"div\", {\"class\": \"title\"})\n",
    "    for post in posts:\n",
    "        l = post.text\n",
    "        cleaned = l.replace('“', '').replace('”','')\n",
    "        company_list.append(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all dates that company was affected with ransomware \n",
    "date_list = []\n",
    "for link in webpage_links:\n",
    "    res = requests.get(link)\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    posts = soup.findAll(\"div\", {\"class\": \"footer\"})\n",
    "    for post in posts:\n",
    "        l = post.text\n",
    "        split_string = l.split(\",\", 1)\n",
    "        substring = split_string[0]\n",
    "        cleaned = substring.replace('\\n','')\n",
    "        date_list.append(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placing victims into a DataFrame\n",
    "df = pd.DataFrame(columns=['Company Name', 'Industry', 'Affected_Date'])\n",
    "df['Company Name'] = company_list\n",
    "df['Affected_Date'] = date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Selenium to automate finding industries\n",
    "socket.socket = old_socket # reuse the original socket for selenium\n",
    "s=Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=s)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glassdoor search\n",
    "industry_list = []\n",
    "location_list = []\n",
    "number = 0\n",
    "\n",
    "for header in df['Company name']: \n",
    "    try:\n",
    "        driver.get('https://www.google.com')\n",
    "        search = driver.find_element(By.NAME, 'q')\n",
    "        search.send_keys(header + ' glassdoor overview')\n",
    "        search.send_keys(Keys.RETURN)\n",
    "        glassdoor_page = driver.find_element_by_tag_name('h3') # clicking the first search result\n",
    "        glassdoor_page.click()\n",
    "        \n",
    "        if \"glassdoor\" in driver.current_url:     \n",
    "            pagesource = driver.page_source\n",
    "            soup = BeautifulSoup(pagesource, 'html.parser')\n",
    "            labels = soup.find_all('label')\n",
    "            for label in labels:\n",
    "                if (label.text == \"Industry:\"):\n",
    "                    industry = label.next_sibling.text\n",
    "                    industry_list.append(industry)\n",
    "                    break\n",
    "            else:\n",
    "                industry_list.append(np.nan)\n",
    "                \n",
    "            for label in labels:\n",
    "                if (label.text == \"Headquarters:\"):\n",
    "                    location = label.next_sibling.text\n",
    "                    location_list.append(location)\n",
    "                    break\n",
    "            else:\n",
    "                location_list.append(np.nan)\n",
    "        else:\n",
    "            industry_list.append(np.nan)\n",
    "            location_list.append(np.nan)\n",
    "    except Exception as e:\n",
    "        industry_list.append(np.nan)\n",
    "        location_list.append(np.nan)\n",
    "        print(e)\n",
    "\n",
    "    print(str(number) + \") \" + header)\n",
    "    number = number + 1\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placing all scraped data into a DataFrame\n",
    "df['Industry'] = industry_list\n",
    "df['Headquarters'] = location_list\n",
    "\n",
    "# Cleaning data\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True) # reset index\n",
    "\n",
    "df.to_csv('Conti.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding countires of headquarters\n",
    "data = pd.read_csv('Conti.csv')\n",
    "geolocator = Nominatim(user_agent = \"geoapiExercises\")\n",
    "country_list = []\n",
    "\n",
    "for city in data['Headquarters']:\n",
    "    place, (lat, lng) = geolocator.geocode(city)\n",
    "    country = place.split()[-1]\n",
    "    country_list.append(country)\n",
    "    \n",
    "data['Country'] = country_list\n",
    "df.to_csv('Conti.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Scraping Pysa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary\n",
    "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigating to Pysa\n",
    "os.popen(r'/home/kali/.local/share/torbrowser/tbb/x86_64/tor-browser_en-US/Browser/TorBrowser/Tor/tor')\n",
    "\n",
    "binary=FirefoxBinary(r'/home/kali/.local/share/torbrowser/tbb/x86_64/tor-browser_en-US/Browser/firefox')\n",
    "fp=FirefoxProfile(r'/home/kali/.local/share/torbrowser/tbb/x86_64/tor-browser_en-US/Browser/TorBrowser/Data/Browser/profile.default/')\n",
    "\n",
    "fp.set_preference('extensions.torlauncher.start_tor',False)#note this\n",
    "fp.set_preference('network.proxy.type',1)\n",
    "fp.set_preference('network.proxy.socks', '127.0.0.1')\n",
    "fp.set_preference('network.proxy.socks_port', 9150)\n",
    "fp.set_preference(\"network.proxy.socks_remote_dns\", True)\n",
    "fp.update_preferences()\n",
    "\n",
    "driver = webdriver.Firefox(firefox_profile=fp,firefox_binary=binary)\n",
    "driver.get('http://pysa2bitc5ldeyfak4seeruqymqs4sj5wt5qkcq7aoyg4h2acqieywad.onion/partners.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining vitcims from Pysa\n",
    "elements = driver.find_elements_by_class_name('page-header')\n",
    "companyList = []\n",
    "webList = []\n",
    "for i in elements:\n",
    "    element = i.find_elements_by_xpath('.//span')\n",
    "    companyList.append(element[0].text)\n",
    "    webList.append(element[2].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placing Pysa victims into a DataFrame\n",
    "df = pd.DataFrame({\"Company Name\":companyList, \"Website\": webList})\n",
    "df['Industry'] = np.nan\n",
    "df['Location'] = np.nan\n",
    "df.to_csv(\"Pysa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Selenium to automate finding of industries and headquarters\n",
    "socks.set_default_proxy(socks.SOCKS5, \"localhost\", 9150)\n",
    "old_socket = socket.socket # saving old socket info \n",
    "socket.socket = socks.socksocket\n",
    "socket.socket = old_socket # reuse the original socket for selenium\n",
    "s=Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=s)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glassdoor search\n",
    "industry_list = []\n",
    "location_list = []\n",
    "number = 0\n",
    "\n",
    "for i in df[\"Company Name\"]: \n",
    "    try:\n",
    "        driver.get(\"https://www.google.com/search?q=\" + i + \" glassdoor overview\")\n",
    "        \n",
    "        glassdoor_page = driver.find_element_by_tag_name('h3') # clicking the first search result\n",
    "        glassdoor_page.click()\n",
    "        \n",
    "        if \"glassdoor\" in driver.current_url:\n",
    "            pagesource = driver.page_source\n",
    "            soup = BeautifulSoup(pagesource, 'html.parser')\n",
    "            labels = soup.find_all('label')\n",
    "            for label in labels:\n",
    "                if (label.text == \"Industry:\"):\n",
    "                    industry = label.next_sibling.text\n",
    "                    industry_list.append(industry)\n",
    "                    break\n",
    "            else:\n",
    "                industry_list.append(np.nan)\n",
    "                \n",
    "            for label in labels:\n",
    "                if (label.text == \"Headquarters:\"):\n",
    "                    location = label.next_sibling.text\n",
    "                    location_list.append(location)\n",
    "                    break\n",
    "            else:\n",
    "                location_list.append(np.nan)\n",
    "        else:\n",
    "            company_list.append(np.nan)\n",
    "            industry_list.append(np.nan)\n",
    "            location_list.append(np.nan)\n",
    "    except Exception as e:\n",
    "        company_list.append(np.nan)\n",
    "        industry_list.append(np.nan)\n",
    "        location_list.append(np.nan)\n",
    "        print(e)\n",
    "\n",
    "    print(str(number) + \") \" + i + \" | \" + industry + \" | \" + location)\n",
    "    number = number + 1\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placing scraped data into DataFrame\n",
    "df['Industry'] = industry_list\n",
    "df['Headquarters'] = location_list\n",
    "\n",
    "df.to_csv('PysaWithNA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using selenium to automate scraping of industries and headquarters using LinkedIn\n",
    "socks.set_default_proxy(socks.SOCKS5, \"localhost\", 9150)\n",
    "socket.socket = socket.socket\n",
    "s=Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to LinkedIn with throwaway account\n",
    "email = \"lando.ozan@zooants.com\"\n",
    "password = \"password123\"\n",
    "\n",
    "driver.get('https://www.linkedin.com/login')\n",
    "time.sleep(3)\n",
    "\n",
    "driver.find_element_by_id('username').send_keys(email)\n",
    "driver.find_element_by_id('password').send_keys(password)\n",
    "driver.find_element_by_id('password').send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinkedIn search\n",
    "index = 0\n",
    "x = np.nan\n",
    "\n",
    "for company in df[\"Company Name\"]: \n",
    "    if ( pd.isnull(df.loc[index, \"Industry\"]) or pd.isnull(df.loc[index, \"Headquarters\"])):\n",
    "        print(company)\n",
    "        try:\n",
    "            driver.get(\"https://www.google.com/search?q=\" + company + \" linkedin\")\n",
    "            results = driver.find_elements_by_css_selector('div.g')\n",
    "            link = results[0].find_element_by_tag_name(\"a\")\n",
    "            href = link.get_attribute(\"href\")\n",
    "            driver.get(href + \"/about\") \n",
    "        except:\n",
    "            print(\"Access Error\")\n",
    "            continue\n",
    "        \n",
    "        if (pd.isnull(df.loc[index, \"Industry\"])):\n",
    "            try:\n",
    "                industry = driver.find_element_by_xpath('//dt[text()=\"Industry\"]/following-sibling::dd')\n",
    "                df.loc[index, \"Industry\"] = industry.text\n",
    "            except:\n",
    "                df.loc[index, \"Industry\"] = \"NA\"\n",
    "        if (pd.isnull(df.loc[index, \"Headquarters\"])):\n",
    "            try:\n",
    "                hq = driver.find_element_by_xpath('//dt[text()=\"Headquarters\"]/following-sibling::dd')\n",
    "                df.loc[index, \"Headquarters\"] = hq.text\n",
    "            except:\n",
    "                df.loc[index, \"Headquarters\"] = \"NA\"\n",
    "    \n",
    "\n",
    "    index = index + 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving DataFrame to CSV file\n",
    "df.to_csv('Pysa.csv')\n",
    "\n",
    "# Finding countires of headquarters\n",
    "data = pd.read_csv('Pysa.csv')\n",
    "geolocator = Nominatim(user_agent = \"geoapiExercises\")\n",
    "country_list = []\n",
    "\n",
    "for city in data['Headquarters']:\n",
    "    place, (lat, lng) = geolocator.geocode(city)\n",
    "    country = place.split()[-1]\n",
    "    country_list.append(country)\n",
    "    \n",
    "data['Country'] = country_list\n",
    "df.to_csv('Pysa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Scraping XINGTeam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing XINGTeam link\n",
    "XING_link = \"http://xingnewj6m4qytljhfwemngm7r7rogrindbq7wrfeepejgxc3bwci7qd.onion\"\n",
    "res = requests.get(XING_link)\n",
    "soup = BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all links out of the soup and deleting None's\n",
    "\n",
    "links = [link.get('href') for link in soup.find_all('a', href=True)]\n",
    "links = list(filter(None, links))\n",
    "\n",
    "webpage_links = [] \n",
    "for l in links:\n",
    "    l = XING_link + l\n",
    "    webpage_links.append(l)\n",
    "        \n",
    "webpage_links = webpage_links[1:-1] # remvoving first and last element from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all victim companies\n",
    "info_list = []\n",
    "\n",
    "for link in webpage_links:\n",
    "    row_info = []\n",
    "    res = requests.get(link)\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    dt_tags = soup.find_all('dt')\n",
    "    dd_tags = soup.find_all('dd')\n",
    "\n",
    "    for tag in dd_tags:\n",
    "        row_info.append(tag.text)\n",
    "    info_list.append(row_info)\n",
    "\n",
    "info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placing scraped data into a DataFrame\n",
    "df = pd.DataFrame(info_list)\n",
    "df.columns = ['Company Name', 'Total Revenue', 'Volume of Stolen Data', 'Published', 'Last Updated', 'Info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automating finding of industries and headquarters using Selenium\n",
    "s=Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=s)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glassdoor search\n",
    "company_list = []\n",
    "industry_list = []\n",
    "location_list = []\n",
    "number = 0\n",
    "\n",
    "for company in df['Company Name']: \n",
    "    try:\n",
    "        driver.get('https://www.google.com')\n",
    "        search = driver.find_element(By.NAME, 'q')\n",
    "        search.send_keys(company + ' glassdoor \"overview\"')\n",
    "        search.send_keys(Keys.RETURN)\n",
    "        glassdoor_page = driver.find_element_by_tag_name('h3') # clicking the first search result\n",
    "        glassdoor_page.click()\n",
    "        \n",
    "        if \"glassdoor\" in driver.current_url:\n",
    "            pagesource = driver.page_source\n",
    "            soup = BeautifulSoup(pagesource, 'html.parser')\n",
    "            labels = soup.find_all('label')\n",
    "            for label in labels:\n",
    "                if (label.text == \"Industry:\"):\n",
    "                    industry = label.next_sibling.text\n",
    "                    industry_list.append(industry)\n",
    "                    break\n",
    "            else:\n",
    "                industry_list.append(np.nan)\n",
    "                \n",
    "            for label in labels:\n",
    "                if (label.text == \"Headquarters:\"):\n",
    "                    location = label.next_sibling.text\n",
    "                    location_list.append(location)\n",
    "                    break\n",
    "            else:\n",
    "                location_list.append(np.nan)\n",
    "        else:\n",
    "            industry_list.append(np.nan)\n",
    "            location_list.append(np.nan)\n",
    "    except Exception as e:\n",
    "        industry_list.append(np.nan)\n",
    "        location_list.append(np.nan)\n",
    "        print(e)\n",
    "\n",
    "    print(str(number) + \") \" + company)\n",
    "    number = number + 1\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing data in DataFrame + Cleaning data\n",
    "new_df = pd.DataFrame(columns=['Company Name', 'Industry', 'Headquarters', 'Date of attack'])\n",
    "\n",
    "new_df['Company Name'] = df['Company Name']\n",
    "new_df['Industry'] = industry_list\n",
    "new_df['Headquarters'] = location_list\n",
    "new_df['Date of attack'] = df['Published']\n",
    "\n",
    "new_df = new_df.dropna()\n",
    "new_df = new_df.reset_index(drop=True) # reset index\n",
    "\n",
    "new_df.to_csv('XINGTeam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding countries of victim companies\n",
    "data = pd.read_csv('XINGTeam.csv')\n",
    "\n",
    "geolocator = Nominatim(user_agent = \"geoapiExercises\")\n",
    "num = 0\n",
    "country_list = []\n",
    "\n",
    "for city in data['Headquarters']:\n",
    "    place, (lat, lng) = geolocator.geocode(city)\n",
    "    country = place.split()[-1]\n",
    "    country_list.append(country)\n",
    "    \n",
    "data['Country'] = country_list\n",
    "\n",
    "data.to_csv(\"XINGTeam_Final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Scraping BlackMatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing BlackMatter\n",
    "blackmatter_link = \"http://blackmax7su6mbwtcyo3xwtpfxpm356jjqrs34y4crcytpw7mifuedyd.onion/\"\n",
    "res = requests.get(blackmatter_link)\n",
    "soup = BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all victim companies\n",
    "blackmatter_links = [\"http://blackmax7su6mbwtcyo3xwtpfxpm356jjqrs34y4crcytpw7mifuedyd.onion/\",\n",
    "                    \"http://blackmax7su6mbwtcyo3xwtpfxpm356jjqrs34y4crcytpw7mifuedyd.onion/?page=2&per-page=18\"]\n",
    "\n",
    "header_list = []\n",
    "\n",
    "for link in blackmatter_links:\n",
    "    res = requests.get(link)\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    h4_tags = soup.find_all('h4')\n",
    "\n",
    "    for tag in h4_tags:\n",
    "        header_list.append(tag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data into DataFrame\n",
    "df = pd.DataFrame(columns=['BlackMatter Headers', 'Company Name', 'Industry', 'Headquarters'])\n",
    "df['BlackMatter Headers'] = header_list\n",
    "\n",
    "df.to_csv('BlackMatter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Selenium to automate fidning of industries + headquarters\n",
    "s=Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=s)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glassdoor search\n",
    "company_list = []\n",
    "industry_list = []\n",
    "location_list = []\n",
    "number = 0\n",
    "\n",
    "for header in header_list: \n",
    "    try:\n",
    "        driver.get('https://www.google.com')\n",
    "        search = driver.find_element(By.NAME, 'q')\n",
    "        search.send_keys(header + ' glassdoor \"overview\"')\n",
    "        search.send_keys(Keys.RETURN)\n",
    "        glassdoor_page = driver.find_element_by_tag_name('h3') # clicking the first search result\n",
    "        glassdoor_page.click()\n",
    "        \n",
    "        if \"glassdoor\" in driver.current_url:\n",
    "            company = driver.find_element_by_tag_name('h1')\n",
    "            company_list.append(company.text)\n",
    "            \n",
    "            pagesource = driver.page_source\n",
    "            soup = BeautifulSoup(pagesource, 'html.parser')\n",
    "            labels = soup.find_all('label')\n",
    "            for label in labels:\n",
    "                if (label.text == \"Industry:\"):\n",
    "                    industry = label.next_sibling.text\n",
    "                    industry_list.append(industry)\n",
    "                    break\n",
    "            else:\n",
    "                industry_list.append(np.nan)\n",
    "                \n",
    "            for label in labels:\n",
    "                if (label.text == \"Headquarters:\"):\n",
    "                    location = label.next_sibling.text\n",
    "                    location_list.append(location)\n",
    "                    break\n",
    "            else:\n",
    "                location_list.append(np.nan)\n",
    "        else:\n",
    "            company_list.append(np.nan)\n",
    "            industry_list.append(np.nan)\n",
    "            location_list.append(np.nan)\n",
    "    except Exception as e:\n",
    "        company_list.append(np.nan)\n",
    "        industry_list.append(np.nan)\n",
    "        location_list.append(np.nan)\n",
    "        print(e)\n",
    "\n",
    "    print(str(number) + \") \" + header)\n",
    "    number = number + 1\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing scraped data into DataFrame + Cleaning data\n",
    "df['BlackMatter Headers'] = header_list\n",
    "df['Company Name'] = company_list\n",
    "df['Industry'] = industry_list\n",
    "df['Headquarters'] = location_list\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop=True) # reset index\n",
    "df\n",
    "\n",
    "df.to_csv('BlackMatter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding countries of victim companies\n",
    "data = pd.read_csv('BlackMatter.csv')\n",
    "\n",
    "geolocator = Nominatim(user_agent = \"geoapiExercises\")\n",
    "num = 0\n",
    "country_list = []\n",
    "\n",
    "for city in data['Headquarters']:\n",
    "    place, (lat, lng) = geolocator.geocode(city)\n",
    "    country = place.split()[-1]\n",
    "    country_list.append(country)\n",
    "    \n",
    "data['Country']= country_list\n",
    "data.to_csv(\"BlackMatter.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Scraping RansomExx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accesing RansomExx\n",
    "url = \"http://rnsm777cdsjrsdlbs4v5qoeppu3px6sb2igmh53jzrx7ipcrbjz5b2ad.onion/\"\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all links out of the soup and filter out companies websites\n",
    "links = [link.get('href') for link in soup.find_all('a', href=True)]\n",
    "links = list(filter(lambda link: link.startswith('/'),links))\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving all links into a list\n",
    "webpage_links = [] \n",
    "for l in links:\n",
    "    if \"page\" in l:\n",
    "        l = url[:-1] + l\n",
    "        webpage_links.append(l)\n",
    "        \n",
    "webpage_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding by class name\n",
    "posts = soup.find_all('h5',class_='card-title')\n",
    "for post in posts:\n",
    "    title = post.text\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing data in a DataFrame\n",
    "df = pd.DataFrame()\n",
    "publishedDate = np.nan\n",
    "visitCount = np.nan\n",
    "leakSize = np.nan\n",
    "\n",
    "for link in webpage_links:\n",
    "    res = requests.get(link)\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    for content in soup.find_all('div',class_='card-body'):\n",
    "        Information = []\n",
    "        companyName = content.find('h5',class_='card-title').text\n",
    "        companyLink = content.find_all('p',class_='card-text')[0].a['href']\n",
    "        companySummary = content.find_all('p',class_='card-text')[1].text\n",
    "        detail = content.find_all('p',class_='card-text')[2].text\n",
    "        print('Name:',companyName)\n",
    "        print('Link:',companyLink)\n",
    "        print('Summary: ',companySummary)\n",
    "        print('Detail: ', detail)\n",
    "        print('----------------------------------------------------------------------\\n')\n",
    "        \n",
    "        # Retrieve info from detail\n",
    "        print(detail)\n",
    "        for i in detail.split(', '):\n",
    "            j = i.split(': ')\n",
    "            if j[0] == 'published':\n",
    "                publishedDate = j[1]\n",
    "            elif j[0] == 'visits':\n",
    "                visitCount = j[1]\n",
    "            elif j[0] == 'leak size':\n",
    "                leakSize = j[1]\n",
    "\n",
    "        newRow = {'Company Name': companyName, 'Website': companyLink, 'Summary': companySummary,\n",
    "                  'Publised Date': publishedDate, 'Visit Count': visitCount, 'Leak Size': leakSize}\n",
    "        df = df.append(newRow, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glassdoor search to search for industries + headquarters\n",
    "industry_list = []\n",
    "headquarter_list = []\n",
    "number = 0\n",
    "\n",
    "for company in df['Company Name']: \n",
    "    try:\n",
    "        driver.get('https://www.google.com')\n",
    "        search = driver.find_element(By.NAME, 'q')\n",
    "        search.send_keys(company + ' glassdoor overview')\n",
    "        search.send_keys(Keys.RETURN)\n",
    "        glassdoor_page = driver.find_element_by_tag_name('h3') # clicking the first search result\n",
    "        glassdoor_page.click()\n",
    "        \n",
    "        pagesource = driver.page_source\n",
    "        soup = BeautifulSoup(pagesource, 'html.parser')\n",
    "        labels = soup.find_all('label')\n",
    "        print(str(number) + \") \" + company + ': \\n')\n",
    "        \n",
    "        for label in labels:\n",
    "            if (label.text == \"Industry:\"):\n",
    "                industry = label.next_sibling.text\n",
    "                industry_list.append(industry)\n",
    "                print('Industry: ',industry)\n",
    "                break\n",
    "        else:\n",
    "            industry_list.append(np.nan)\n",
    "            print('No industry')\n",
    "            \n",
    "        for label in labels:\n",
    "            if (label.text == \"Headquarters:\"):\n",
    "                headquarter = label.next_sibling.text\n",
    "                headquarter_list.append(headquarter)\n",
    "                print('Headquarter: ', headquarter)\n",
    "                break\n",
    "        else:\n",
    "            headquarter_list.append(np.nan)\n",
    "            print('No headquarter')\n",
    "        \n",
    "        print('--------------------')\n",
    "\n",
    "    except:\n",
    "        industry_list.append(np.nan)\n",
    "        headquarter_list.append(np.nan)\n",
    "        print(str(number) + \") \" + company + \": \" + str(np.nan) + \"except loop\")\n",
    "    number = number + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing industries + headquarters into the DataFrame\n",
    "df['Industry'] = industry_list\n",
    "df['Headquarter'] = headquarter_list\n",
    "df.to_csv('RansomEXX.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding countries of victim companies\n",
    "data = pd.read_csv('RansomEXX.csv')\n",
    "geolocator = Nominatim(user_agent = \"geoapiExercises\")\n",
    "num = 0\n",
    "country_list=[]\n",
    "\n",
    "for city in data['Headquarter']:\n",
    "    place, (lat, lng) = geolocator.geocode(city)\n",
    "    country = place.split()[-1]\n",
    "    country_list.append(country)\n",
    "    \n",
    "data['Country']= country_list\n",
    "data.to_csv('RansomEXX.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
