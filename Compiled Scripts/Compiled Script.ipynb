{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import socks\n",
    "import socket\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "socks.set_default_proxy(socks.SOCKS5, \"localhost\", 9150)\n",
    "old_socket = socket.socket # saving old socket info \n",
    "socket.socket = socks.socksocket\n",
    "def getaddrinfo(*args):\n",
    "    return [(socket.AF_INET, socket.SOCK_STREAM, 6, '', (args[0], args[1]))]\n",
    "socket.getaddrinfo = getaddrinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Scraping REvil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REvil_link = \"http://dnpscnbaix6nkwvystl3yxglz7nteicqrou3t75tpcc5532cztc46qyd.onion\"\n",
    "res = requests.get(REvil_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting all weblinks into a list \n",
    "links = [link.get('href') for link in soup.find_all('a', href=True)]\n",
    "links = list(filter(None, links))\n",
    "\n",
    "webpage_links = [] \n",
    "for l in links:\n",
    "    if \"page\" in l:\n",
    "        l = REvil_link + l\n",
    "        webpage_links.append(l)\n",
    "        \n",
    "webpage_links = webpage_links[1:-1] # remvoving first and last element from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of victims from REvil\n",
    "REvilHeader_list = []\n",
    "for link in webpage_links:\n",
    "    res = requests.get(link)\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    posts = soup.find_all(class_ = \"blog-post-title\")\n",
    "    \n",
    "    for post in posts:\n",
    "        title = post.find('a')\n",
    "        REvilHeader_list.append(title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placing victims into a DataFrame \n",
    "df = pd.DataFrame(columns=['REvil Headers', 'Company Name', 'Industry', 'Headquarters'])\n",
    "df['REvil Headers'] = REvilHeader_list\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Selenium to automate finding industries\n",
    "socket.socket = old_socket # reuse the original socket for selenium\n",
    "s=Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=s)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glassdoor search\n",
    "company_list = []\n",
    "industry_list = []\n",
    "location_list = []\n",
    "number = 0\n",
    "\n",
    "for header in REvilHeader_list: \n",
    "    try:\n",
    "        driver.get('https://www.google.com')\n",
    "        search = driver.find_element(By.NAME, 'q')\n",
    "        search.send_keys(header + ' glassdoor overview')\n",
    "        search.send_keys(Keys.RETURN)\n",
    "        glassdoor_page = driver.find_element_by_tag_name('h3') # clicking the first search result\n",
    "        glassdoor_page.click()\n",
    "        \n",
    "        if \"glassdoor\" in driver.current_url:\n",
    "            company = driver.find_element_by_tag_name('h1')\n",
    "            company_list.append(company.text)\n",
    "            \n",
    "            pagesource = driver.page_source\n",
    "            soup = BeautifulSoup(pagesource, 'html.parser')\n",
    "            labels = soup.find_all('label')\n",
    "            for label in labels:\n",
    "                if (label.text == \"Industry:\"):\n",
    "                    industry = label.next_sibling.text\n",
    "                    industry_list.append(industry)\n",
    "                    break\n",
    "            else:\n",
    "                industry_list.append(np.nan)\n",
    "                \n",
    "            for label in labels:\n",
    "                if (label.text == \"Headquarters:\"):\n",
    "                    location = label.next_sibling.text\n",
    "                    location_list.append(location)\n",
    "                    break\n",
    "            else:\n",
    "                location_list.append(np.nan)\n",
    "        else:\n",
    "            company_list.append(np.nan)\n",
    "            industry_list.append(np.nan)\n",
    "            location_list.append(np.nan)\n",
    "    except Exception as e:\n",
    "        company_list.append(np.nan)\n",
    "        industry_list.append(np.nan)\n",
    "        location_list.append(np.nan)\n",
    "        print(e)\n",
    "\n",
    "    print(str(number) + \") \" + header)\n",
    "    number = number + 1\n",
    "    \n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
